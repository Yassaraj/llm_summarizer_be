{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import HTML, display\n",
        "\n",
        "def set_css():\n",
        "  display(HTML('''\n",
        "  <style>\n",
        "    pre {\n",
        "        white-space: pre-wrap;\n",
        "    }\n",
        "  </style>\n",
        "  '''))\n",
        "get_ipython().events.register('pre_run_cell', set_css)"
      ],
      "metadata": {
        "id": "EH3FzQlzU6t_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6Y9jgnMRGkZn",
        "outputId": "3340c80d-aa09-4b52-a0e6-aeedb825c106"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pymupdf\n",
            "  Downloading pymupdf-1.25.3-cp39-abi3-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (3.4 kB)\n",
            "Downloading pymupdf-1.25.3-cp39-abi3-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (20.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m20.0/20.0 MB\u001b[0m \u001b[31m60.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pymupdf\n",
            "Successfully installed pymupdf-1.25.3\n"
          ]
        }
      ],
      "source": [
        "!pip install pymupdf\n",
        "\n",
        "import fitz  # PyMuPDF\n",
        "import google.generativeai as genai\n",
        "import json"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_text_and_images(pdf_path):\n",
        "    doc = fitz.open(pdf_path)\n",
        "    pages_data = []\n",
        "\n",
        "    for page_num in range(len(doc)):\n",
        "        page = doc[page_num]\n",
        "        text = page.get_text(\"text\")\n",
        "        num_images = len(page.get_images(full=True))\n",
        "        pages_data.append({\"page_num\": page_num + 1, \"content\": text, \"num_images\": num_images})\n",
        "\n",
        "    return pages_data"
      ],
      "metadata": {
        "id": "22UkC7EOGl0N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pdf_path = \"/content/drive/MyDrive/Literature Review_13th_Oct.docx.pdf\"\n",
        "pdf_data = extract_text_and_images(pdf_path)\n",
        "print(pdf_data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KKG6dZt0L_2Z",
        "outputId": "c8e25a72-4126-44c5-84ba-88f05225b87a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{'page_num': 1, 'content': 'Literature Review:\\nAssumptions:\\n1. Proposal is completed and ready to submitted by 19th Oct 2023\\n2. That’s means you have done initial literature review\\nFind good research paper:\\n1. Put the topic or keywords that you are planning to research in semantics scholar\\nhttps://www.semanticscholar.org/\\n2. So it will list all the research that is relevant to what you are looking for\\n3. Filter for last 5 years\\n4. Check for the credibility of the research paper\\na. Journal – check for the impact score – higher the impact score it is better\\nThe following journals are normally good:\\n.IEEE journals\\n.Science Direct journals\\n.ACM journals\\n.Elsevier Journal\\ni.Web of Science\\xa0\\nb. H Index the author is applicable for both journal and conference proceedings\\n.Check authors h index – most of the time author is the student who\\njointly publishes theirs work with supervisor (coauthor) if both have\\npoor h index better not to consider\\xa0\\ni.If H index of the author is not good then check H Index of the\\ncoauthor (mostly supervisors)\\nb. No of citations is another way measure the goodness or credibility of the\\nresearch paper\\n.But older the paper better the no of citation\\n.New ones even if it is good may not have good no of citation\\nb. Authors affiliations also can used to measure the credibility of the research\\npaper\\xa0\\n0.\\nUse the above criteria to select good research papers\\xa0\\n0.\\nWhen you select a paper\\n1. Read abstract, => if you find it relevant\\n2. Read the introduction => if you still find it relevant\\n3. Read the conclusion\\xa0\\na.\\nInitially read Abstract => introduction => conclusion\\xa0\\na.\\nIf you still find it relevant read the whole research papers (first time if you do not\\nunder, read for second time still you don’t understand read for the third time) Read and\\nreread because research papers are not written the story tellers and very technical so it\\nmight be hard to read.\\n1\\n', 'num_images': 0}, {'page_num': 2, 'content': 'b.\\nWhen reading research paper select the ratio of 60%:40% between Conference\\nproceeding: journal article [60:40, 40:60, 50:50]\\nAs the first step of the Literature Review:\\n1. Literature Survey\\nCreate a table where the you can have several columns:\\n1. Citation\\n2. Summary of their work\\n3. Technology used (further divide based on whatever that is relevant)\\na. Dataset\\nb. Preprocessing techniques\\nc.\\nFeature selection\\nd. Feature engineering\\ne. Algorithm selection – what they did to select the algorithm\\nf.\\nHyper parameter tunining\\ng. Evaluation metrics\\nh. Benchmarking\\n4. Limitations\\n5. Contribution\\nWhen you read a research paper => for each research paper fill the table => find a sample\\ntemplate here\\nYou can finetune the above to make it more appropriate for your research.\\nProcess of Literature Survey:\\n1. Read a paper that is relevant to your research\\n2. Fill the table you are maintaining for LS\\n3. Add the relevant things into the Concept Map / Concept Graph\\n4. Then create a separate table for each subtopics in the Technology used\\nDataset\\nDataset\\nPros\\nCons\\nCitation\\n2\\n', 'num_images': 0}, {'page_num': 3, 'content': 'Preprocessing technique\\nProcessing\\ntechnique\\nPros\\nCons\\nCitation\\nAlgorithm selection\\nProcessing\\ntechnique\\nPros\\nCons\\nCitation\\nSVM\\nInitially you fill this table with new techniques or dataset or whatever it is identified e.g.\\nwhen you find a particular algorithm has been used in the past work\\n1. Fill the LS table\\n2. Add that algorithm that you will be maintaining to keep track of algorithm\\ntable\\n3. Other than the first column others can be later\\nWe were talking about Literature Survey. What is the difference between Literature Survey\\nand Literature Review?\\nLiterature Survey is summarizing someone else work.\\nLiterature Review = Literature Survey + Critiquing\\nCritiquing:\\n1. Personal opinion\\n2. Objectively evaluating (fair and unbiased)\\n3. A systematic evaluation (scientific manner)\\n4. Identifying the strengths and the weaknesses\\nLiterature review isn’t about summarizing what they have stated but critiquing i.e your\\nopinion but scientific manner with facts and objectively to identify pros and cons of their\\nwork.\\nSo as part of the LR never writing about summary instead cite (There is no reason why we\\nshould summarize what they have stated instead just cite) bring in your opinion\\n3\\n', 'num_images': 0}, {'page_num': 4, 'content': 'In your entire thesis nowhere, you must put definitions, theories (unless otherwise you\\nhave come up with your own theory as part of the research). Everything in your thesis\\nmust be your own opinions, works and findings.\\nIf you want to put a definition just cite. Machine learning (abc 2023, pg 135)\\nWhen you are writing the LR following are the subtopics of the chapter:\\n2.1 Chapter Overview\\n2.2 Concept Map or Concept Graph\\n2.3 Problem Domain - ? Code Development and Maintenance?, ?Sinhala new articles?, ?\\nPersonalized Learning Material Recommendation?\\nWhen writing the problem in the LR there must be any repetition of what is written as part\\nof Problem Domain in the Chapter 01.\\nIn chapter 01 problem domain is written in a very brief manner whereas in LR you will go\\nin-depth into the problem domain\\nThis is the last topic to be written though it is the third subtopic in the LR chapter\\n2.3.n The last topic of Problem Domain is “Proposed Architecture of the system you are\\nbuilding” if it Personalized Learning Material Recommendation then “Proposed Architecture\\nof Personalized Learning Material Recommendation”\\nMaximum no of page must be 3 page +/- 1\\nn meaning year is the last topic refer to Thiloshon’s thesis page number 14\\n2.4Existing work\\nWhen doing LR this is the first thing to start with\\nLiterature survey => Literature Review (Critiquing)\\nHow many works you will be putting here?\\nAs much as you can but must not exceed four pages +/- 1 page\\nThis where critiquing matters and this where how much you write doesn’t\\nmatter but how much you cite and critique matters.\\nTotal LR must not cross 15 pages +/- 10%\\n4\\n', 'num_images': 0}, {'page_num': 5, 'content': 'You can discuss by dividing the existing work into meaningful subtopics.\\nThen summarize the existing works under each sub topic\\n2.5 Problem Solving Approaches / Technological Review / Algorithmic Review\\nThis where the table we created on each step in implementing our system will be\\nuseful\\nFor example for a data science project following steps of implementation of model\\ncan be used\\n2.5.1\\nDataset\\nNo dataset is available and you will be collecting your own dataset then discuss\\nabout it briefly and also discuss how you are going to validate the dataset – the\\nlabeling, etc..\\n2.5.2\\nPreprocessing\\n2.5.3\\nFeature Engineering\\n2.5.4\\nFeature Extraction / feature selection\\n2.5.5\\nAlgorithm selection\\n2.5.6\\nTraining and testing\\n2.5.7\\nEvaluating the model (Evaluation metrics)\\n2.5.8\\nBenchmarking\\n2.6 Chapter Summary\\nWe would have done a very comprehensive Literature Survey on Existing work or let it be\\nsummaring or tabularizing the technological review (Tables that was talking about end of\\npage number 02 and beginning of page no 3) where are we going to put this?\\nIf you have done a very comprehensive work on summarizing and tabularizing are we going\\nto throw it away\\nPut it appendix because there is not limit for number of pages for appendix\\nWhen you complete the proposal you have completed two chapters of the thesis:\\nChapter 01: Introduction (Chapter 01: Problem in the project proposal)\\nChapter 02: Literature Review\\nChapter 03: Methodology (Chapter 02: Methodology in the project proposal)\\nMain topics in the Chapter 02 Literature Review:\\n2.1 Chapter Overview\\n5\\n', 'num_images': 0}, {'page_num': 6, 'content': '2.2 Concept Map / Graph\\n2.3 Problem Domain\\n2.4 Existing work\\n2.5 Technological Review\\n2.6 Chapter Summary\\nWhen doing the LR:\\nStep 01: Review of Existing work\\n1.1 Literature Survey\\n1.2 Literature Review\\nDeadline: 20th Oct 2023\\nStep 02: Technological Review\\nDeadline: 27th Oct 2023\\nStep 03: Problem Domain\\nDeadline: 3rd Nov 2023\\nFinal version of LR chapter:\\nDeadline: 10th Nov 2023\\n6\\n', 'num_images': 0}]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(pdf_data[0][\"content\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UR2D6FRAPXAl",
        "outputId": "417fc004-da29-45b8-b36f-f86fe1cd007b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Literature Review:\n",
            "Assumptions:\n",
            "1. Proposal is completed and ready to submitted by 19th Oct 2023\n",
            "2. That’s means you have done initial literature review\n",
            "Find good research paper:\n",
            "1. Put the topic or keywords that you are planning to research in semantics scholar\n",
            "https://www.semanticscholar.org/\n",
            "2. So it will list all the research that is relevant to what you are looking for\n",
            "3. Filter for last 5 years\n",
            "4. Check for the credibility of the research paper\n",
            "a. Journal – check for the impact score – higher the impact score it is better\n",
            "The following journals are normally good:\n",
            ".IEEE journals\n",
            ".Science Direct journals\n",
            ".ACM journals\n",
            ".Elsevier Journal\n",
            "i.Web of Science \n",
            "b. H Index the author is applicable for both journal and conference proceedings\n",
            ".Check authors h index – most of the time author is the student who\n",
            "jointly publishes theirs work with supervisor (coauthor) if both have\n",
            "poor h index better not to consider \n",
            "i.If H index of the author is not good then check H Index of the\n",
            "coauthor (mostly supervisors)\n",
            "b. No of citations is another way measure the goodness or credibility of the\n",
            "research paper\n",
            ".But older the paper better the no of citation\n",
            ".New ones even if it is good may not have good no of citation\n",
            "b. Authors affiliations also can used to measure the credibility of the research\n",
            "paper \n",
            "0.\n",
            "Use the above criteria to select good research papers \n",
            "0.\n",
            "When you select a paper\n",
            "1. Read abstract, => if you find it relevant\n",
            "2. Read the introduction => if you still find it relevant\n",
            "3. Read the conclusion \n",
            "a.\n",
            "Initially read Abstract => introduction => conclusion \n",
            "a.\n",
            "If you still find it relevant read the whole research papers (first time if you do not\n",
            "under, read for second time still you don’t understand read for the third time) Read and\n",
            "reread because research papers are not written the story tellers and very technical so it\n",
            "might be hard to read.\n",
            "1\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google import genai\n",
        "\n",
        "def get_gemini_response(prompt):\n",
        "    client = genai.Client(api_key=\"\")\n",
        "\n",
        "    response = client.models.generate_content(\n",
        "        model=\"gemini-2.0-flash\",\n",
        "        contents=prompt,\n",
        "    )\n",
        "\n",
        "    return(response.text)"
      ],
      "metadata": {
        "id": "emeGQ340GpQN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt = f\"From the following text, extract the 5 most relevant keyword, and generate a concise summary (max 80 words) ensuring at least 4 of the extracted keywords appear in the page summary\"\n",
        "text = pdf_data[0][\"content\"]\n",
        "prompt = f\"From the following text, Generate a concise summary under 80 words relevant to the domain of the text and also axtract the 5 most relevant keywords ensuring at least 4 of the extracted keywords appear in the page summary as well. Provide the output as a JSON with the keys as summery and keywords. For example:{{'summary': 'extracted summary', 'keywords': ['keyword1', 'keyword2', ...]}} . Make sure to just return the JSON. Do not include any additional characters surround the JSON object Text:{text}\"\n",
        "response = get_gemini_response(prompt)\n",
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        },
        "id": "v61xoHD9OX8n",
        "outputId": "6b70e433-cf68-43ef-d8f9-c7b2f4da90d5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "```json\n",
            "{\n",
            "  \"summary\": \"This guide outlines strategies for conducting a literature review. Key steps include using Semantic Scholar with relevant keywords, filtering for recent research, and assessing paper credibility. Credibility checks involve evaluating the journal's impact score (IEEE, Science Direct, ACM, Elsevier), author's H-index, citation count, and author affiliations. Selected papers should be assessed by reading the abstract, introduction, and conclusion to determine relevance before a full read.\",\n",
            "  \"keywords\": [\n",
            "    \"literature review\",\n",
            "    \"research paper\",\n",
            "    \"credibility\",\n",
            "    \"H-index\",\n",
            "    \"citations\"\n",
            "  ]\n",
            "}\n",
            "```\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "def extract_json(text):\n",
        "    start = text.find(\"{\")\n",
        "    end = text.rfind(\"}\")\n",
        "\n",
        "    if start == -1 or end == -1 or start > end:\n",
        "        return None  # No valid JSON found\n",
        "\n",
        "    json_str = text[start:end+1]\n",
        "\n",
        "    try:\n",
        "        return json.loads(json_str)  # Convert to dictionary\n",
        "    except json.JSONDecodeError:\n",
        "        return None"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "DMvI5E7tOYMd",
        "outputId": "c7983d75-cf97-4e7d-8c75-9b483e292f1d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def retry_function(max_retries=3):\n",
        "    for attempt in range(max_retries):\n",
        "        response = get_gemini_response(prompt)\n",
        "        if extract_json(text) is not None:\n",
        "            return response\n",
        "        print(f\"Attempt {attempt + 1}: Invalid response, retrying...\")\n",
        "    return None"
      ],
      "metadata": {
        "id": "XifJ-8Xhe9VR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response_dictionary = extract_json(response)\n",
        "print(response_dictionary)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "id": "reWUkuSwb-RT",
        "outputId": "67207f1f-ccc8-4f89-f17b-ab573dbd6ae8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'summary': \"This guide outlines strategies for conducting a literature review. Key steps include using Semantic Scholar with relevant keywords, filtering for recent research, and assessing paper credibility. Credibility checks involve evaluating the journal's impact score (IEEE, Science Direct, ACM, Elsevier), author's H-index, citation count, and author affiliations. Selected papers should be assessed by reading the abstract, introduction, and conclusion to determine relevance before a full read.\", 'keywords': ['literature review', 'research paper', 'credibility', 'H-index', 'citations']}\n"
          ]
        }
      ]
    }
  ]
}